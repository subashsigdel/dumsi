import nest_asyncio
import asyncio
import edge_tts
from IPython.display import Audio, display
import numpy as np
from pydub import AudioSegment
import io
import os
import re
import time
import speech_recognition as sr

# Apply nest_asyncio to run asyncio in Jupyter
nest_asyncio.apply()


class NepaliVoiceAssistant:
    def __init__(self, voice="ne-NP-HemkalaNeural", volume_boost=3.0, wake_word="‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä"):
        """
        Initialize the Nepali Voice Assistant

        Parameters:
        - voice: Voice ID (default: ne-NP-HemkalaNeural)
        - volume_boost: How much to increase volume in dB (default: 3.0)
        - wake_word: Word to activate the assistant (default: "‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä" - Helper)
        """
        self.voice = voice
        self.volume_boost = volume_boost
        self.wake_word = wake_word
        self.listening = False

        # Conversation mapping - customize these responses
        self.responses = {
            # Greetings
            "‡§®‡§Æ‡§∏‡•ç‡§§‡•á": ["‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡§∞‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?", "‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§ï‡•á ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?"],
            "‡§π‡•á‡§≤‡•ã": ["‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§ï‡§∏‡§∞‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?", "‡§π‡§ú‡•Å‡§∞, ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç!"],

            # Wake words and activations
            wake_word: ["‡§π‡§ú‡•Å‡§∞, ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç", "‡§Æ ‡§∏‡•Å‡§®‡•ç‡§¶‡•à ‡§õ‡•Å", "‡§π‡§ú‡•Å‡§∞, ‡§ï‡•á ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§õ?"],
            "‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä": ["‡§π‡§ú‡•Å‡§∞, ‡§Æ ‡§Ø‡§π‡§æ‡§Å ‡§õ‡•Å", "‡§∏‡•Å‡§®‡•ç‡§¶‡•à ‡§õ‡•Å"],
            "‡§â‡§†": ["‡§Æ ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å", "‡§π‡§ú‡•Å‡§∞, ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å"],

            # Questions
            "‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ": ["‡§Æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•à ‡§õ‡•Å, ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡§∞‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?", "‡§∏‡§¨‡•à ‡§†‡§ø‡§ï ‡§õ, ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à?"],
            "‡§§‡§ø‡§Æ‡•Ä ‡§ï‡•ã ‡§π‡•å": ["‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•Å‡§Å", "‡§Æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§Æ‡§æ ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•á ‡§è‡§ï ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•Å‡§Å"],

            # Commands
            "‡§∏‡§Æ‡§Ø": ["‡§Ö‡§π‡§ø‡§≤‡•á ‡§∏‡§Æ‡§Ø {current_time} ‡§≠‡§è‡§ï‡•ã ‡§õ"],
            "‡§Æ‡•å‡§∏‡§Æ": ["‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Æ‡•å‡§∏‡§Æ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§π‡•á‡§∞‡•ç‡§® ‡§Æ‡§≤‡§æ‡§à ‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§õ"],

            # Closings
            "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶": ["‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§õ", "‡§ñ‡•Å‡§∂‡•Ä ‡§≤‡§æ‡§ó‡•ç‡§Ø‡•ã ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§® ‡§™‡§æ‡§â‡§Å‡§¶‡§æ"],
            "‡§¨‡§ø‡§¶‡§æ": ["‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ", "‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞, ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§¶‡§ø‡§® ‡§¨‡§ø‡§§‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç"],

            # Default response
            "default": ["‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Æ‡•à‡§≤‡•á ‡§¨‡•Å‡§ù‡§ø‡§®‡§Å", "‡§´‡•á‡§∞‡§ø ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç", "‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§•‡§™ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§π‡•Å‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ?"]
        }

        # Action mapping - for handling specific functions
        self.actions = {
            "‡§∏‡§Æ‡§Ø ‡§¨‡§§‡§æ‡§â": self.tell_time,
            "‡§∏‡§Æ‡§Ø ‡§ï‡§§‡§ø ‡§≠‡§Ø‡•ã": self.tell_time,
            "‡§ò‡§°‡•Ä ‡§π‡•á‡§∞": self.tell_time,

            "‡§ó‡•Ä‡§§ ‡§¨‡§ú‡§æ‡§ä": self.play_music,
            "‡§∏‡§Ç‡§ó‡•Ä‡§§": self.play_music,

            "‡§Æ‡•å‡§∏‡§Æ": self.weather_info,
            "‡§Æ‡•å‡§∏‡§Æ ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ": self.weather_info
        }

        # Initialize the audio enhancement tools
        try:
            import pydub
        except ImportError:
            print("Installing pydub for audio enhancement...")
            import pip
            pip.main(['install', 'pydub'])

        try:
            import speech_recognition
        except ImportError:
            print("Installing speech recognition...")
            import pip
            pip.main(['install', 'SpeechRecognition'])

    async def speak(self, text, output_file="response.mp3"):
        """Generate Nepali TTS with enhanced volume and clarity"""
        try:
            # Format the text with any dynamic content
            formatted_text = self._format_dynamic_content(text)

            # Generate speech using Edge TTS
            communicate = edge_tts.Communicate(text=formatted_text, voice=self.voice)
            await communicate.save(output_file)

            # Enhance the audio
            try:
                # Load the generated audio
                audio = AudioSegment.from_file(output_file)

                # Boost volume
                boosted_audio = audio + self.volume_boost

                # Apply light compression for clarity
                enhanced_audio = self._compress_dynamic_range(boosted_audio)

                # Cut very low frequencies for better clarity
                enhanced_audio = enhanced_audio.high_pass_filter(300)

                # Export the enhanced audio
                enhanced_output = f"enhanced_{output_file}"
                enhanced_audio.export(enhanced_output, format="mp3")

                # Play the audio
                audio_obj = Audio(enhanced_output, autoplay=True)
                display(audio_obj)

                return enhanced_output

            except Exception as e:
                print(f"Enhancement error: {e}")
                # Fallback to original audio
                audio_obj = Audio(output_file, autoplay=True)
                display(audio_obj)
                return output_file

        except Exception as e:
            print(f"Speech generation error: {e}")
            return None

    def _compress_dynamic_range(self, sound, threshold=-20.0, ratio=1.5):
        """Basic dynamic range compression for clearer audio"""

        def gain_computer(dB, threshold, ratio):
            if dB < threshold:
                return 0
            else:
                return (threshold - dB) * (1 - 1 / ratio)

        # Process the audio
        samples = np.array(sound.get_array_of_samples())
        # Convert to float & normalize
        samples = samples.astype(np.float32) / np.iinfo(np.int16).max

        # Calculate dB
        dB = 20 * np.log10(np.abs(samples) + 1e-10)
        gain_reduction = np.array([gain_computer(d, threshold, ratio) for d in dB])
        gain_reduction_linear = 10 ** (gain_reduction / 20)
        compressed_samples = samples * gain_reduction_linear

        # Convert back to int16
        compressed_samples = (compressed_samples * np.iinfo(np.int16).max).astype(np.int16)

        # Create a new audio segment
        compressed_sound = sound._spawn(compressed_samples.tobytes())
        return compressed_sound

    def _format_dynamic_content(self, text):
        """Replace placeholders with dynamic content"""
        if "{current_time}" in text:
            import datetime
            # Format time in Nepali style
            now = datetime.datetime.now()
            time_str = now.strftime("%I:%M %p")
            text = text.replace("{current_time}", time_str)
        return text

    async def listen(self):
        """Listen for user input - simplified version for demo"""
        # In a real implementation, this would use a microphone
        # For now, we'll simulate with text input
        print("üé§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§¨‡•ã‡§≤‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç (‡§µ‡§æ ‡§ü‡§æ‡§á‡§™ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç):")
        user_input = input()
        return user_input

    async def process_command(self, text):
        """Process the user's command and determine response"""
        # Check for direct matches in responses dictionary
        for key, responses in self.responses.items():
            if key.lower() in text.lower():
                # Get a random response from the list
                import random
                response = random.choice(responses)
                await self.speak(response)
                return True

        # Check for action triggers
        for key, action_func in self.actions.items():
            if key.lower() in text.lower():
                await action_func()
                return True

        # If no match found, use default response
        import random
        default = random.choice(self.responses["default"])
        await self.speak(default)
        return False

    # Action functions
    async def tell_time(self):
        """Tell the current time in Nepali"""
        import datetime
        now = datetime.datetime.now()
        # Format time in Nepali style
        time_str = now.strftime("%I:%M %p")

        response = f"‡§Ö‡§π‡§ø‡§≤‡•á ‡§∏‡§Æ‡§Ø {time_str} ‡§≠‡§è‡§ï‡•ã ‡§õ"
        await self.speak(response)

    async def play_music(self):
        """Simulate playing music"""
        await self.speak("‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Ö‡§π‡§ø‡§≤‡•á ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§¨‡§ú‡§æ‡§â‡§®‡•á ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§õ‡•à‡§®")

    async def weather_info(self):
        """Simulate providing weather information"""
        await self.speak("‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Æ‡•å‡§∏‡§Æ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§π‡•á‡§∞‡•ç‡§® ‡§Æ‡§≤‡§æ‡§à ‡§á‡§®‡•ç‡§ü‡§∞‡§®‡•á‡§ü ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§õ")

    async def run(self):
        """Run the voice assistant in continuous mode"""
        await self.speak("‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•Å‡§Å‡•§ ‡§Æ‡§≤‡§æ‡§à ‡§¨‡•ã‡§≤‡§æ‡§â‡§® '{}' ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§".format(self.wake_word))

        self.listening = False

        while True:
            # Wait for user input
            user_input = await self.listen()

            # Check if this is a wake word when not already listening
            if not self.listening:
                if self.wake_word.lower() in user_input.lower():
                    self.listening = True
                    wake_responses = self.responses.get(self.wake_word, ["‡§π‡§ú‡•Å‡§∞, ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç"])
                    import random
                    await self.speak(random.choice(wake_responses))
                continue

            # Process the command
            if user_input.lower() in ["‡§¨‡§®‡•ç‡§¶ ‡§ó‡§∞", "‡§¨‡§ø‡§¶‡§æ", "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶"]:
                await self.speak("‡§†‡§ø‡§ï ‡§õ, ‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ")
                self.listening = False
                continue

            # Otherwise process the command
            await self.process_command(user_input)

    def add_custom_response(self, trigger, responses):
        """Add or modify custom responses to the assistant

        Parameters:
        - trigger: The text that triggers this response
        - responses: List of possible responses to choose from
        """
        if not isinstance(responses, list):
            responses = [responses]  # Convert single string to list

        self.responses[trigger] = responses
        print(f"Added response for '{trigger}'")

    def add_custom_action(self, trigger, action_function):
        """Add a custom action to the assistant

        Parameters:
        - trigger: The text that triggers this action
        - action_function: Async function to call when triggered
        """
        self.actions[trigger] = action_function
        print(f"Added action for '{trigger}'")


# Example usage
async def main():
    # Create the assistant
    assistant = NepaliVoiceAssistant(
        voice="ne-NP-HemkalaNeural",
        volume_boost=4.0,
        wake_word="‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä"
    )

    # Add some custom responses
    assistant.add_custom_response("‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ", ["‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§≠‡•á‡§ü‡•ç‡§® ‡§™‡§æ‡§â‡§Å‡§¶‡§æ ‡§ñ‡•Å‡§∏‡•Ä ‡§≤‡§æ‡§ó‡•ç‡§Ø‡•ã"])
    assistant.add_custom_response("‡§Æ‡•å‡§∏‡§Æ ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ", ["‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§∏‡§´‡§æ ‡§õ", "‡§Ü‡§ú ‡§ò‡§æ‡§Æ ‡§≤‡§æ‡§ó‡•á‡§ï‡•ã ‡§õ"])

    # Example of adding a custom action (must be async)
    async def tell_joke():
        await assistant.speak(
            "‡§è‡§â‡§ü‡§æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ú‡•ã‡§ï: ‡§è‡§ï ‡§ú‡§®‡§æ ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á ‡§π‡•ã‡§ü‡§≤‡§Æ‡§æ ‡§ó‡§è ‡§Ö‡§®‡§ø ‡§≠‡§®‡•á, '‡§ö‡§ø‡§Ø‡§æ ‡§ñ‡§æ‡§®‡•Å ‡§π‡•ã‡§≤‡§æ!'. ‡§π‡•ã‡§ü‡•á‡§≤ ‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ï‡§≤‡•á ‡§≠‡§®‡•á, '‡§ö‡§ø‡§Ø‡§æ ‡§™‡§ø‡§â‡§®‡•Å ‡§π‡•Å‡§®‡•ç‡§õ ‡§ï‡§ø ‡§ñ‡§æ‡§®‡•Å ‡§π‡•Å‡§®‡•ç‡§õ?'")

    assistant.add_custom_action("‡§ú‡•ã‡§ï ‡§∏‡•Å‡§®‡§æ‡§â", tell_joke)

    # Test with some examples
    print("\nüëã ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§‡§Æ‡•ç ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä (Nepali Voice Assistant)")
    print("---------------------------------------------")

    # Test basic responses
    print("\nüîä Testing basic greeting:")
    await assistant.speak("‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•Å‡§Å‡•§ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡§∞‡•Ä ‡§Æ‡§¶‡•ç‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?")

    # Test time function
    print("\nüîä Testing time function:")
    await assistant.tell_time()

    # Example conversation loop
    print("\nüîä Starting conversation mode (type '‡§¨‡§®‡•ç‡§¶ ‡§ó‡§∞' to exit):")
    print("---------------------------------------------")
    print("‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä (to wake up the assistant)")
    print("‡§§‡§ø‡§Æ‡•Ä ‡§ï‡•ã ‡§π‡•å? (to ask who the assistant is)")
    print("‡§∏‡§Æ‡§Ø ‡§ï‡§§‡§ø ‡§≠‡§Ø‡•ã? (to ask the time)")
    print("‡§ú‡•ã‡§ï ‡§∏‡•Å‡§®‡§æ‡§â (to hear a joke)")
    print("‡§Æ‡•å‡§∏‡§Æ ‡§ï‡§∏‡•ç‡§§‡•ã ‡§õ? (to ask about weather)")
    print("‡§¨‡§®‡•ç‡§¶ ‡§ó‡§∞ (to exit)")

    # Uncomment to run in interactive mode
    # await assistant.run()

    # For demo purposes, simulate a conversation
    print("\nüîä Demo conversation:")
    await assistant.process_command("‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä")
    time.sleep(1)
    await assistant.process_command("‡§§‡§ø‡§Æ‡•Ä ‡§ï‡•ã ‡§π‡•å")
    time.sleep(1)
    await assistant.process_command("‡§∏‡§Æ‡§Ø ‡§ï‡§§‡§ø ‡§≠‡§Ø‡•ã")
    time.sleep(1)
    await assistant.process_command("‡§ú‡•ã‡§ï ‡§∏‡•Å‡§®‡§æ‡§â")
    time.sleep(1)
    await assistant.process_command("‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶")


# Run the assistant
if __name__ == "__main__" or 'ipykernel' in sys.modules:
    asyncio.run(main())